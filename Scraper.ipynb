{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93699317-b59f-4326-a537-95b1d242e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4f4ca1-ddd3-40e2-81f0-7117868a4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_the_page_to_load(driver,wait):\n",
    "    title = driver.title\n",
    "    try : \n",
    "        wait.until(\n",
    "            lambda d : d.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "    except :\n",
    "        print(f'Page : \"{title}\" is not able to download.')\n",
    "    else :\n",
    "        print(f'Successfully loaded \"{title}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3636208-bbab-4b47-a68c-7d1af2ea58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n",
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n",
      "Clicked to utmost right.\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[0;32m---> 82\u001b[0m     wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mvisibility_of(text))\n\u001b[1;32m     83\u001b[0m     scroll_to_element_smoothly(driver, text)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScrolled to footer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/support/wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "# website is able to detect that some bot is scraping data and preventinbg it to scrape it to handle this we write below code using option class\n",
    "# each browser has his own option class\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "# user agent makes it look that the request is coming from another browser not a bot\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "driver.maximize_window()  \n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver,5)\n",
    "url = 'https://www.99acres.com/'\n",
    "\n",
    "driver.get(url)\n",
    "loading_wait(driver,wait)\n",
    "\n",
    "try : \n",
    "    search_bar = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH,\"//input[@id='keyword2']\"))\n",
    "    )\n",
    "except : \n",
    "    print(f'Timeout while locating search bar.')\n",
    "else :\n",
    "    search_bar.send_keys(\"Chennai\")\n",
    "    time.sleep(2)\n",
    "    loading_wait(driver,wait)\n",
    "\n",
    "try : \n",
    "    location = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'//*[@id=\"0\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(f'Could not find the entered location')\n",
    "else :\n",
    "    location.click()\n",
    "    time.sleep(1)\n",
    "    search = driver.find_element(By.XPATH,'//*[@id=\"searchform_search_btn\"]')\n",
    "    search.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "try : \n",
    "    slider = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(f'Could not Slide.')\n",
    "else :\n",
    "    actions = ActionChains(driver)\n",
    "    (\n",
    "        actions\n",
    "        .click_and_hold(slider)\n",
    "        .move_by_offset(-73,0)\n",
    "        .release()\n",
    "        .perform()\n",
    "    )\n",
    "    \n",
    "verified = driver.find_element(By.XPATH,\"//span[normalize-space()='Verified']\")\n",
    "verified.click()\n",
    "ready_to_move = driver.find_element(By.XPATH,\"//span[normalize-space()='Ready To Move']\")\n",
    "ready_to_move.click()\n",
    "\n",
    "while True :\n",
    "    try : \n",
    "        right_click = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH,\"//i[@class='iconS_Common_24 icon_upArrow cc__rightArrow']\"))\n",
    "        )\n",
    "    except :\n",
    "        print('Clicked to utmost right.')\n",
    "        break\n",
    "    else :\n",
    "        right_click.click()    \n",
    "\n",
    "photos = driver.find_element(By.XPATH,\"//span[normalize-space()='With Photos']\")\n",
    "photos.click()\n",
    "videos = driver.find_element(By.XPATH,\"//span[normalize-space()='With Videos']\")\n",
    "videos.click()\n",
    "time.sleep(5)\n",
    "\n",
    "def scroll_to_element_smoothly(driver, element):\n",
    "    driver.execute_script(\"\"\"\n",
    "        arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\n",
    "    \"\"\", element)\n",
    "    time.sleep(2)\n",
    "\n",
    "text = driver.find_element(By.XPATH,\"//div[@class='fatfooter__colr title_bold']\")\n",
    "time.sleep(5)\n",
    "if text:\n",
    "    wait.until(EC.visibility_of(text))\n",
    "    scroll_to_element_smoothly(driver, text)\n",
    "    print(\"Scrolled to footer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80437cbc-502f-4f48-b3f0-77e889b84722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "HTTPConnectionPool(host='localhost', port=59429): Read timed out. (read timeout=120)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/http/client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1430\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch button is not clickable within given time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m---> 56\u001b[0m     search_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m     57\u001b[0m     wait_for_the_page_to_load(driver ,wait)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# fixing the budget slider (click , hold , drag , release) at 5cr\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/webelement.py:119\u001b[0m, in \u001b[0;36mWebElement.click\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    >>> element.click()\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mCLICK_ELEMENT)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/webelement.py:572\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    570\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    571\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:427\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    425\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    842\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read_error(error):\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# Read retry?\u001b[39;00m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m reraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         read \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;129;01min\u001b[39;00m _blocking_errnos:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='localhost', port=59429): Read timed out. (read timeout=120)"
     ]
    }
   ],
   "source": [
    "# website is able to detect that some bot is scraping data and preventinbg it to scrape it to handle this we write below code using option class\n",
    "# each browser has his own option class\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "# user agent makes it look that the request is coming from another browser not a bot\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "# accessing the target webpage\n",
    "url = \"https://www.99acres.com/\"\n",
    "driver.get(url)\n",
    "# wait for the page to load\n",
    "wait_for_the_page_to_load(driver ,wait)\n",
    "\n",
    "# identifying the search bar\n",
    "try : \n",
    "    search_bar = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH,'//*[@id=\"keyword2\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating searchbar.\\n\")\n",
    "else : \n",
    "    search_bar.send_keys(\"Chennai\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# selecting valid options from list\n",
    "try :\n",
    "    valid_option = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"0\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating valid search option.\\n\")\n",
    "else :\n",
    "    valid_option.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "# click on search button \n",
    "try : \n",
    "    search_button = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"searchform_search_btn\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Search button is not clickable within given time\")\n",
    "else :\n",
    "    search_button.click()\n",
    "    wait_for_the_page_to_load(driver ,wait)\n",
    "\n",
    "# fixing the budget slider (click , hold , drag , release) at 5cr\n",
    "try : \n",
    "    slider = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating the slider(circle)\")\n",
    "else :\n",
    "    # move_by_offset is to drag ( move - drag , offset - by how much pixels ) , .perform() is used after every action method to perform all the mentioned actions\n",
    "    actions = ActionChains(driver)\n",
    "    (\n",
    "        actions\n",
    "        .click_and_hold(slider)\n",
    "        .move_by_offset(-73,0) # we have to move in left so negative value and not vertically so y axis is 0         # -73 because at -73 it will reach at 5 cr (calculated experimentally)\n",
    "        .release()\n",
    "        .perform()\n",
    "    )\n",
    "    time.sleep(2)\n",
    "\n",
    "# filtering results\n",
    "# 1. verified\n",
    "try :\n",
    "    verified = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while clicking the verified option\")\n",
    "else :\n",
    "    verified.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "# 2. Ready to move\n",
    "ready_to_move = wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH , '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    ")\n",
    "ready_to_move.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# moving to the right side to unhide remaining filters\n",
    "while True : \n",
    "    try :\n",
    "        filter_right_button = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH,\"//i[@class='iconS_Common_24 icon_upArrow cc__rightArrow']\"))\n",
    "    )\n",
    "    except :\n",
    "        print(\"Timeout because we have uncovered all the filters.\\n\")\n",
    "        break\n",
    "    else :\n",
    "        filter_right_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "# With Photos\n",
    "with_photos = wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    ")\n",
    "with_photos.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# With Videos\n",
    "with_videos =  wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    ")\n",
    "with_videos.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# NAVIGATING PAGES AND EXTRACTING(SCRAPING) DATA\n",
    "\n",
    "data = []\n",
    "page_count = 0\n",
    "while True :\n",
    "    page_count += 1\n",
    "    try :\n",
    "        time.sleep(5)\n",
    "        next_page_button = driver.find_element(By.XPATH,\"//a[normalize-space()='Next Page >']\")\n",
    "    except :\n",
    "        print(f\"Timeout because we have navigated all the {page_count} pages\")\n",
    "        break # to come out of loop\n",
    "    else :\n",
    "        try :\n",
    "            # scrool down the page till this button is visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block : 'center'});\", next_page_button)\n",
    "             # getBoundingClientRect() helps to scroll the page till the particular part or element (helps to scroll element wise rather than pixel wise)\n",
    "            # scroll till its 100 pixel from the top of the page (-100) this helps to ensure the stability while locating the element\n",
    "            time.sleep(2)\n",
    "\n",
    "            time.sleep(2)\n",
    "            wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH , \"//a[normalize-space()='Next Page >']\"))\n",
    "            ).click()\n",
    "            time.sleep(8) # giving time for the next page to load and reache to next page option\n",
    "        except :\n",
    "            print(\"Timeout while clicking on next page.\\n\")\n",
    "            \n",
    "            \n",
    "\n",
    "# SCRAPING data from the last page (because as we reach the last page it will not so next page button and we will break out of the loop without scraping last page data so to scrape last page data as well we have explixitly do it)\n",
    "                   \n",
    "rows = driver.find_elements(By.CLASS_NAME , \"PseudoTupleRevamp__outerTupleWrap\")\n",
    "for row in rows:\n",
    "    # property name\n",
    "    try :\n",
    "        name = row.find_element(By.CLASS_NAME , \"tupleNew__headingNrera\").text\n",
    "    except :\n",
    "        name = np.nan\n",
    "                \n",
    "    # property location\n",
    "    try :\n",
    "        location = row.find_element(By.CLASS_NAME , \"tupleNew__propType\").text\n",
    "    except :\n",
    "        location = np.nan\n",
    "                \n",
    "    # property price  \n",
    "    try :\n",
    "        price = row.find_element(By.CLASS_NAME , \"tupleNew__priceValWrap\").text\n",
    "    except :\n",
    "        price = np.nan\n",
    "\n",
    "    # property area elements\n",
    "    try :\n",
    "        elements = row.find_elements(By.CLASS_NAME , \"tupleNew__area1Type\")\n",
    "    except :\n",
    "        area , bhk =[np.nan, np.nan]\n",
    "    else :\n",
    "        area , bhk =[ele.text for ele in elements]\n",
    "\n",
    "                    \n",
    "    property = {\n",
    "        \"name\" : name,\n",
    "        \"location\" : location,\n",
    "        \"price\" : price,\n",
    "        \"area\" : area,\n",
    "        \"bhk\" : bhk\n",
    "    }\n",
    "    data.append(property)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e704bf-bd20-45f7-b054-7840cef3be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True :\n",
    "    page_count += 1\n",
    "    try :\n",
    "        next_page = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH,\"//a[normalize-space()='Next Page >']\"))\n",
    "            )\n",
    "        if next_page : \n",
    "            print('found')\n",
    "    except : \n",
    "        print(f'Timeout. Navigated {page_count} pages.')\n",
    "        break\n",
    "    else :\n",
    "        try : \n",
    "            driver.execute_script('window.scrollBy(0,arguments[0].getBoundingClientRect.top()-100);',page_count)\n",
    "\n",
    "            wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH,\"//a[normalize-space()='Next Page >']\"))\n",
    "            ).click()\n",
    "            \n",
    "        except : \n",
    "            print(f'Could\"t scroll.')\n",
    "            break\n",
    "            \n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webScraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
