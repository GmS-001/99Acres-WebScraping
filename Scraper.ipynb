{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93699317-b59f-4326-a537-95b1d242e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4f4ca1-ddd3-40e2-81f0-7117868a4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_the_page_to_load(driver,wait):\n",
    "    title = driver.title\n",
    "    try : \n",
    "        wait.until(\n",
    "            lambda d : d.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "    except :\n",
    "        print(f'Page : \"{title}\" is not able to download.')\n",
    "    else :\n",
    "        print(f'Successfully loaded \"{title}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3636208-bbab-4b47-a68c-7d1af2ea58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n",
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n",
      "Clicked to utmost right.\n",
      "Timeout while clicking on next page.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# website is able to detect that some bot is scraping data and preventinbg it to scrape it to handle this we write below code using option class\n",
    "# each browser has his own option class\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "# user agent makes it look that the request is coming from another browser not a bot\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "driver.maximize_window()  \n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver,10)\n",
    "url = 'https://www.99acres.com/'\n",
    "\n",
    "driver.get(url)\n",
    "wait_for_the_page_to_load(driver,wait)\n",
    "\n",
    "try : \n",
    "    search_bar = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH,\"//input[@id='keyword2']\"))\n",
    "    )\n",
    "except : \n",
    "    print(f'Timeout while locating search bar.')\n",
    "else :\n",
    "    search_bar.send_keys(\"Chennai\")\n",
    "    time.sleep(2)\n",
    "    wait_for_the_page_to_load(driver,wait)\n",
    "\n",
    "try : \n",
    "    location = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'//*[@id=\"0\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(f'Could not find the entered location')\n",
    "else :\n",
    "    location.click()\n",
    "    time.sleep(1)\n",
    "    search = driver.find_element(By.XPATH,'//*[@id=\"searchform_search_btn\"]')\n",
    "    search.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "try : \n",
    "    slider = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(f'Could not Slide.')\n",
    "else :\n",
    "    actions = ActionChains(driver)\n",
    "    (\n",
    "        actions\n",
    "        .click_and_hold(slider)\n",
    "        .move_by_offset(-73,0)\n",
    "        .release()\n",
    "        .perform()\n",
    "    )\n",
    "    \n",
    "verified = driver.find_element(By.XPATH,\"//span[normalize-space()='Verified']\")\n",
    "verified.click()\n",
    "ready_to_move = driver.find_element(By.XPATH,\"//span[normalize-space()='Ready To Move']\")\n",
    "ready_to_move.click()\n",
    "\n",
    "while True :\n",
    "    try : \n",
    "        right_click = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH,\"//i[@class='iconS_Common_24 icon_upArrow cc__rightArrow']\"))\n",
    "        )\n",
    "    except :\n",
    "        print('Clicked to utmost right.')\n",
    "        break\n",
    "    else :\n",
    "        right_click.click()    \n",
    "\n",
    "photos = driver.find_element(By.XPATH,\"//span[normalize-space()='With Photos']\")\n",
    "photos.click()\n",
    "videos = driver.find_element(By.XPATH,\"//span[normalize-space()='With Videos']\")\n",
    "videos.click()\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "page_count = 0\n",
    "while True :\n",
    "    page_count += 1\n",
    "    try :\n",
    "        time.sleep(5)\n",
    "        next_page_button = driver.find_element(By.XPATH,\"//a[normalize-space()='Next Page >']\")\n",
    "    except :\n",
    "        print(f\"Timeout because we have navigated all the {page_count} pages\")\n",
    "        break # to come out of loop\n",
    "    else :\n",
    "        try :\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            # scrool down the page till this button is visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block : 'center'});\", next_page_button)\n",
    "             # getBoundingClientRect() helps to scroll the page till the particular part or element (helps to scroll element wise rather than pixel wise)\n",
    "            # scroll till its 100 pixel from the top of the page (-100) this helps to ensure the stability while locating the element\n",
    "            time.sleep(2)\n",
    "            \n",
    "            names = driver.find_elements(By.CLASS_NAME, \"tupleNew__outerTupleWrap undefined \")\n",
    "            print(names[0])\n",
    "            print(len(names))\n",
    "            print(type(names))\n",
    "            print(type(content))\n",
    "            break\n",
    "            \n",
    "\n",
    "            \n",
    "            time.sleep(2)\n",
    "            wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH , \"//a[normalize-space()='Next Page >']\"))\n",
    "            ).click()\n",
    "            time.sleep(8) # giving time for the next page to load and reache to next page option\n",
    "        except :\n",
    "            print(\"Timeout while clicking on next page.\\n\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80437cbc-502f-4f48-b3f0-77e889b84722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\".\n",
      "Successfully loaded \"Property in Chennai - Real Estate in Chennai\".\n",
      "Timeout because we have uncovered all the filters.\n",
      "\n",
      "Timeout while clicking on next page.\n",
      "\n",
      "Timeout because we have navigated all the 7 pages\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id\nStacktrace:\n0   chromedriver                        0x0000000102fdee10 cxxbridge1$str$ptr + 2817040\n1   chromedriver                        0x0000000102fd70ac cxxbridge1$str$ptr + 2784940\n2   chromedriver                        0x0000000102b1e74c cxxbridge1$string$len + 92632\n3   chromedriver                        0x0000000102b58ec8 cxxbridge1$string$len + 332116\n4   chromedriver                        0x0000000102b8129c cxxbridge1$string$len + 496936\n5   chromedriver                        0x0000000102b805a4 cxxbridge1$string$len + 493616\n6   chromedriver                        0x0000000102aee9d4 chromedriver + 92628\n7   chromedriver                        0x0000000102fa3c98 cxxbridge1$str$ptr + 2575000\n8   chromedriver                        0x0000000102fa6f64 cxxbridge1$str$ptr + 2588004\n9   chromedriver                        0x0000000102f83a20 cxxbridge1$str$ptr + 2443296\n10  chromedriver                        0x0000000102fa77e0 cxxbridge1$str$ptr + 2590176\n11  chromedriver                        0x0000000102f74b14 cxxbridge1$str$ptr + 2382100\n12  chromedriver                        0x0000000102aecc68 chromedriver + 85096\n13  dyld                                0x00000001834d5058 start + 2224\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 156\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout while clicking on next page.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# SCRAPING data from the last page (because as we reach the last page it will not so next page button and we will break out of the loop without scraping last page data so to scrape last page data as well we have explixitly do it)\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m rows \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPseudoTupleRevamp__outerTupleWrap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# property name\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m :\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:936\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_script(find_element_js, by\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[1;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/webScraping/lib/python3.13/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mInvalidSessionIdException\u001b[0m: Message: invalid session id\nStacktrace:\n0   chromedriver                        0x0000000102fdee10 cxxbridge1$str$ptr + 2817040\n1   chromedriver                        0x0000000102fd70ac cxxbridge1$str$ptr + 2784940\n2   chromedriver                        0x0000000102b1e74c cxxbridge1$string$len + 92632\n3   chromedriver                        0x0000000102b58ec8 cxxbridge1$string$len + 332116\n4   chromedriver                        0x0000000102b8129c cxxbridge1$string$len + 496936\n5   chromedriver                        0x0000000102b805a4 cxxbridge1$string$len + 493616\n6   chromedriver                        0x0000000102aee9d4 chromedriver + 92628\n7   chromedriver                        0x0000000102fa3c98 cxxbridge1$str$ptr + 2575000\n8   chromedriver                        0x0000000102fa6f64 cxxbridge1$str$ptr + 2588004\n9   chromedriver                        0x0000000102f83a20 cxxbridge1$str$ptr + 2443296\n10  chromedriver                        0x0000000102fa77e0 cxxbridge1$str$ptr + 2590176\n11  chromedriver                        0x0000000102f74b14 cxxbridge1$str$ptr + 2382100\n12  chromedriver                        0x0000000102aecc68 chromedriver + 85096\n13  dyld                                0x00000001834d5058 start + 2224\n"
     ]
    }
   ],
   "source": [
    "# website is able to detect that some bot is scraping data and preventinbg it to scrape it to handle this we write below code using option class\n",
    "# each browser has his own option class\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "# user agent makes it look that the request is coming from another browser not a bot\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "# accessing the target webpage\n",
    "url = \"https://www.99acres.com/\"\n",
    "driver.get(url)\n",
    "# wait for the page to load\n",
    "wait_for_the_page_to_load(driver ,wait)\n",
    "\n",
    "# identifying the search bar\n",
    "try : \n",
    "    search_bar = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH,'//*[@id=\"keyword2\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating searchbar.\\n\")\n",
    "else : \n",
    "    search_bar.send_keys(\"Chennai\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# selecting valid options from list\n",
    "try :\n",
    "    valid_option = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"0\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating valid search option.\\n\")\n",
    "else :\n",
    "    valid_option.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "# click on search button \n",
    "try : \n",
    "    search_button = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"searchform_search_btn\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Search button is not clickable within given time\")\n",
    "else :\n",
    "    search_button.click()\n",
    "    wait_for_the_page_to_load(driver ,wait)\n",
    "\n",
    "# fixing the budget slider (click , hold , drag , release) at 5cr\n",
    "try : \n",
    "    slider = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while locating the slider(circle)\")\n",
    "else :\n",
    "    # move_by_offset is to drag ( move - drag , offset - by how much pixels ) , .perform() is used after every action method to perform all the mentioned actions\n",
    "    actions = ActionChains(driver)\n",
    "    (\n",
    "        actions\n",
    "        .click_and_hold(slider)\n",
    "        .move_by_offset(-73,0) # we have to move in left so negative value and not vertically so y axis is 0         # -73 because at -73 it will reach at 5 cr (calculated experimentally)\n",
    "        .release()\n",
    "        .perform()\n",
    "    )\n",
    "    time.sleep(2)\n",
    "\n",
    "# filtering results\n",
    "# 1. verified\n",
    "try :\n",
    "    verified = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    "    )\n",
    "except :\n",
    "    print(\"Timeout while clicking the verified option\")\n",
    "else :\n",
    "    verified.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "# 2. Ready to move\n",
    "ready_to_move = wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH , '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    ")\n",
    "ready_to_move.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# moving to the right side to unhide remaining filters\n",
    "while True : \n",
    "    try :\n",
    "        filter_right_button = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH,\"//i[@class='iconS_Common_24 icon_upArrow cc__rightArrow']\"))\n",
    "    )\n",
    "    except :\n",
    "        print(\"Timeout because we have uncovered all the filters.\\n\")\n",
    "        break\n",
    "    else :\n",
    "        filter_right_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "# With Photos\n",
    "with_photos = wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    ")\n",
    "with_photos.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# With Videos\n",
    "with_videos =  wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH,'/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    ")\n",
    "with_videos.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# NAVIGATING PAGES AND EXTRACTING(SCRAPING) DATA\n",
    "\n",
    "data = []\n",
    "page_count = 0\n",
    "while True :\n",
    "    page_count += 1\n",
    "    try :\n",
    "        time.sleep(5)\n",
    "        next_page_button = driver.find_element(By.XPATH,\"//a[normalize-space()='Next Page >']\")\n",
    "    except :\n",
    "        print(f\"Timeout because we have navigated all the {page_count} pages\")\n",
    "        break # to come out of loop\n",
    "    else :\n",
    "        try :\n",
    "            # scrool down the page till this button is visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block : 'center'});\", next_page_button)\n",
    "             # getBoundingClientRect() helps to scroll the page till the particular part or element (helps to scroll element wise rather than pixel wise)\n",
    "            # scroll till its 100 pixel from the top of the page (-100) this helps to ensure the stability while locating the element\n",
    "            time.sleep(2)\n",
    "\n",
    "            time.sleep(2)\n",
    "            wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH , \"//a[normalize-space()='Next Page >']\"))\n",
    "            ).click()\n",
    "            time.sleep(8) # giving time for the next page to load and reache to next page option\n",
    "        except :\n",
    "            print(\"Timeout while clicking on next page.\\n\")\n",
    "            \n",
    "            \n",
    "\n",
    "# SCRAPING data from the last page (because as we reach the last page it will not so next page button and we will break out of the loop without scraping last page data so to scrape last page data as well we have explixitly do it)\n",
    "                   \n",
    "rows = driver.find_elements(By.CLASS_NAME , \"PseudoTupleRevamp__outerTupleWrap\")\n",
    "for row in rows:\n",
    "    # property name\n",
    "    try :\n",
    "        name = row.find_element(By.CLASS_NAME , \"tupleNew__headingNrera\").text\n",
    "    except :\n",
    "        name = np.nan\n",
    "                \n",
    "    # property location\n",
    "    try :\n",
    "        location = row.find_element(By.CLASS_NAME , \"tupleNew__propType\").text\n",
    "    except :\n",
    "        location = np.nan\n",
    "                \n",
    "    # property price  \n",
    "    try :\n",
    "        price = row.find_element(By.CLASS_NAME , \"tupleNew__priceValWrap\").text\n",
    "    except :\n",
    "        price = np.nan\n",
    "\n",
    "    # property area elements\n",
    "    try :\n",
    "        elements = row.find_elements(By.CLASS_NAME , \"tupleNew__area1Type\")\n",
    "    except :\n",
    "        area , bhk =[np.nan, np.nan]\n",
    "    else :\n",
    "        area , bhk =[ele.text for ele in elements]\n",
    "\n",
    "                    \n",
    "    property = {\n",
    "        \"name\" : name,\n",
    "        \"location\" : location,\n",
    "        \"price\" : price,\n",
    "        \"area\" : area,\n",
    "        \"bhk\" : bhk\n",
    "    }\n",
    "    data.append(property)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.quit() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webScraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
